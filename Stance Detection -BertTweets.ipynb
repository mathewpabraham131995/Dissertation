{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f2752825df9b4efdab000a81c8062aac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7652252cf4944e58f29bdad29347307","IPY_MODEL_ead11630ab654dc8b86d91b59afc0b6c","IPY_MODEL_47e28327a4f340b8a73a741cfbcc5302"],"layout":"IPY_MODEL_093867e8c0104dc3a4552cd3f46a5a46"}},"b7652252cf4944e58f29bdad29347307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79169cbb7edf4325bee5487f7311a1c5","placeholder":"​","style":"IPY_MODEL_8a666a05708a4144856d8fdc772387e6","value":"Downloading (…)lve/main/config.json: 100%"}},"ead11630ab654dc8b86d91b59afc0b6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc0276959f2d49b8b90289da621d1b10","max":614,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf08cac5aec74640baf5db5e727ae787","value":614}},"47e28327a4f340b8a73a741cfbcc5302":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02bbf52faa5e44c4831d23a296846388","placeholder":"​","style":"IPY_MODEL_51e9a7524bfe4368a733985cc466448e","value":" 614/614 [00:00&lt;00:00, 41.8kB/s]"}},"093867e8c0104dc3a4552cd3f46a5a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79169cbb7edf4325bee5487f7311a1c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a666a05708a4144856d8fdc772387e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc0276959f2d49b8b90289da621d1b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf08cac5aec74640baf5db5e727ae787":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02bbf52faa5e44c4831d23a296846388":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e9a7524bfe4368a733985cc466448e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fefb96a1f10414c9e2181301bf9b638":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_088bffa5585e4f3e8a6dd68d9bdd3f30","IPY_MODEL_fe62b2743a88487cbe16b7b463dc272f","IPY_MODEL_fc7090e42e904bd88bcf9645f7531629"],"layout":"IPY_MODEL_4f0ac82be4dd4320b60ee8c062b25d6b"}},"088bffa5585e4f3e8a6dd68d9bdd3f30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d10fcd7bc4924ae8b5b75ecbc49e9a11","placeholder":"​","style":"IPY_MODEL_ce30c31cffee4897aaf8f2558b39d0ff","value":"Downloading (…)olve/main/vocab.json: 100%"}},"fe62b2743a88487cbe16b7b463dc272f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b7b314bc2984f0abf1e78a290e1981e","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36e9c3d0c15c451ea84ba9c1e794237a","value":898823}},"fc7090e42e904bd88bcf9645f7531629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_407c3f5fc4364c97b6039d756af8eedd","placeholder":"​","style":"IPY_MODEL_75e06ea3cab349b2a6296d967982f222","value":" 899k/899k [00:00&lt;00:00, 4.62MB/s]"}},"4f0ac82be4dd4320b60ee8c062b25d6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d10fcd7bc4924ae8b5b75ecbc49e9a11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce30c31cffee4897aaf8f2558b39d0ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b7b314bc2984f0abf1e78a290e1981e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36e9c3d0c15c451ea84ba9c1e794237a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"407c3f5fc4364c97b6039d756af8eedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e06ea3cab349b2a6296d967982f222":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70bc9d76cff24572a1d52c059aaf74c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_050fe5db6161454cab5f9e2ce615931d","IPY_MODEL_434c783b77974b3e81a1d968d84813a2","IPY_MODEL_3cf801737f8646e7bde2e5d3f2dfc8b6"],"layout":"IPY_MODEL_8e69fe98aceb4378830fcd1fef91e0d1"}},"050fe5db6161454cab5f9e2ce615931d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2abed6d30a62449f9e46f3f0ba33a874","placeholder":"​","style":"IPY_MODEL_fc9a1f9c70944142a31256af43376409","value":"Downloading (…)olve/main/merges.txt: 100%"}},"434c783b77974b3e81a1d968d84813a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d96e2b2288fd496f97d271f03b941009","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56dd4abd475a4b6c826fb34ddc6952f5","value":456318}},"3cf801737f8646e7bde2e5d3f2dfc8b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9400e483ddc04753a6e92b319e241504","placeholder":"​","style":"IPY_MODEL_ebb104ab256e4b0abc4d69daa5eab8bc","value":" 456k/456k [00:00&lt;00:00, 2.45MB/s]"}},"8e69fe98aceb4378830fcd1fef91e0d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2abed6d30a62449f9e46f3f0ba33a874":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc9a1f9c70944142a31256af43376409":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d96e2b2288fd496f97d271f03b941009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56dd4abd475a4b6c826fb34ddc6952f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9400e483ddc04753a6e92b319e241504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebb104ab256e4b0abc4d69daa5eab8bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53a5b015646f487589858604f73e43c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46c81bb4790c4d9f80710691d2580e16","IPY_MODEL_bd9ecdda742c443587f61968b2696a47","IPY_MODEL_1dc8a3cc2d67470e878c6ba2d85a49e4"],"layout":"IPY_MODEL_c0005e06544f4d42b33c6f7f44a9f352"}},"46c81bb4790c4d9f80710691d2580e16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d00bccc506fe4dcdab96c1595448c9d1","placeholder":"​","style":"IPY_MODEL_7259ad6e6e3448009241681861d6c961","value":"Downloading (…)/main/tokenizer.json: 100%"}},"bd9ecdda742c443587f61968b2696a47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5910ca80d4564b70b7d5645a5b91410a","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d043af93c20141168c08be8dc28c28d7","value":1355863}},"1dc8a3cc2d67470e878c6ba2d85a49e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08713e84c108490f9130189599be538b","placeholder":"​","style":"IPY_MODEL_21e010d3344349219e90e692bdca546b","value":" 1.36M/1.36M [00:00&lt;00:00, 18.2MB/s]"}},"c0005e06544f4d42b33c6f7f44a9f352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00bccc506fe4dcdab96c1595448c9d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7259ad6e6e3448009241681861d6c961":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5910ca80d4564b70b7d5645a5b91410a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d043af93c20141168c08be8dc28c28d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08713e84c108490f9130189599be538b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21e010d3344349219e90e692bdca546b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"854efc088dab405c8102eb27879572a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41f8f37ed78c4a03848ca8224af4e05f","IPY_MODEL_0c94867ff37c4c5f96f7a04411c8f277","IPY_MODEL_8268163582184d7e86d7c0501661a49c"],"layout":"IPY_MODEL_e6691ddd514d4448bb9cf93aa22b9bde"}},"41f8f37ed78c4a03848ca8224af4e05f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731d6f587e814051872cbb98c591d9b7","placeholder":"​","style":"IPY_MODEL_2db07e9cf45d4a86bad051273bc4c53e","value":"Downloading pytorch_model.bin: 100%"}},"0c94867ff37c4c5f96f7a04411c8f277":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3769d01ee22144a8a4e8bbf831e9ad33","max":1422008553,"min":0,"orientation":"horizontal","style":"IPY_MODEL_478d9f86487742fab73c5e35cd6b0df4","value":1422008553}},"8268163582184d7e86d7c0501661a49c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd1f4af5b0b24743aaa33822fee638a9","placeholder":"​","style":"IPY_MODEL_43c76e67c38640f58ab18469baf0991c","value":" 1.42G/1.42G [00:05&lt;00:00, 269MB/s]"}},"e6691ddd514d4448bb9cf93aa22b9bde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731d6f587e814051872cbb98c591d9b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2db07e9cf45d4a86bad051273bc4c53e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3769d01ee22144a8a4e8bbf831e9ad33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"478d9f86487742fab73c5e35cd6b0df4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd1f4af5b0b24743aaa33822fee638a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43c76e67c38640f58ab18469baf0991c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_1ujeackOK1","outputId":"8110c99e-80f2-4846-d27b-8243bd657ce2","executionInfo":{"status":"ok","timestamp":1691600335996,"user_tz":-60,"elapsed":18189,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkziBkro5xv5","executionInfo":{"status":"ok","timestamp":1691600377189,"user_tz":-60,"elapsed":19806,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"e0fdee14-0994-42b8-8df8-dfd4beda070a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhvnJ2Bgkwjm","outputId":"69a87552-d02f-4be7-edc9-c96b0bdc3142","executionInfo":{"status":"ok","timestamp":1691600404646,"user_tz":-60,"elapsed":5393,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report"],"metadata":{"id":"6uCS_VDok2Ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labelled_tweets = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Final_Labels.csv')\n","\n","train_df, test_df = train_test_split(labelled_tweets, test_size = 0.2, random_state=42)\n","train_df, val_df = train_test_split(train_df, test_size = 0.2, random_state=42)"],"metadata":{"id":"_GeF9zPEnUDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name2 = 'vinai/bertweet-large'\n","tokenizer = AutoTokenizer.from_pretrained(model_name2)\n","model2 = AutoModelForSequenceClassification.from_pretrained(model_name2,num_labels = 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232,"referenced_widgets":["f2752825df9b4efdab000a81c8062aac","b7652252cf4944e58f29bdad29347307","ead11630ab654dc8b86d91b59afc0b6c","47e28327a4f340b8a73a741cfbcc5302","093867e8c0104dc3a4552cd3f46a5a46","79169cbb7edf4325bee5487f7311a1c5","8a666a05708a4144856d8fdc772387e6","cc0276959f2d49b8b90289da621d1b10","bf08cac5aec74640baf5db5e727ae787","02bbf52faa5e44c4831d23a296846388","51e9a7524bfe4368a733985cc466448e","0fefb96a1f10414c9e2181301bf9b638","088bffa5585e4f3e8a6dd68d9bdd3f30","fe62b2743a88487cbe16b7b463dc272f","fc7090e42e904bd88bcf9645f7531629","4f0ac82be4dd4320b60ee8c062b25d6b","d10fcd7bc4924ae8b5b75ecbc49e9a11","ce30c31cffee4897aaf8f2558b39d0ff","7b7b314bc2984f0abf1e78a290e1981e","36e9c3d0c15c451ea84ba9c1e794237a","407c3f5fc4364c97b6039d756af8eedd","75e06ea3cab349b2a6296d967982f222","70bc9d76cff24572a1d52c059aaf74c7","050fe5db6161454cab5f9e2ce615931d","434c783b77974b3e81a1d968d84813a2","3cf801737f8646e7bde2e5d3f2dfc8b6","8e69fe98aceb4378830fcd1fef91e0d1","2abed6d30a62449f9e46f3f0ba33a874","fc9a1f9c70944142a31256af43376409","d96e2b2288fd496f97d271f03b941009","56dd4abd475a4b6c826fb34ddc6952f5","9400e483ddc04753a6e92b319e241504","ebb104ab256e4b0abc4d69daa5eab8bc","53a5b015646f487589858604f73e43c1","46c81bb4790c4d9f80710691d2580e16","bd9ecdda742c443587f61968b2696a47","1dc8a3cc2d67470e878c6ba2d85a49e4","c0005e06544f4d42b33c6f7f44a9f352","d00bccc506fe4dcdab96c1595448c9d1","7259ad6e6e3448009241681861d6c961","5910ca80d4564b70b7d5645a5b91410a","d043af93c20141168c08be8dc28c28d7","08713e84c108490f9130189599be538b","21e010d3344349219e90e692bdca546b","854efc088dab405c8102eb27879572a6","41f8f37ed78c4a03848ca8224af4e05f","0c94867ff37c4c5f96f7a04411c8f277","8268163582184d7e86d7c0501661a49c","e6691ddd514d4448bb9cf93aa22b9bde","731d6f587e814051872cbb98c591d9b7","2db07e9cf45d4a86bad051273bc4c53e","3769d01ee22144a8a4e8bbf831e9ad33","478d9f86487742fab73c5e35cd6b0df4","dd1f4af5b0b24743aaa33822fee638a9","43c76e67c38640f58ab18469baf0991c"]},"id":"KdOjJ5oDnjNZ","outputId":"5ba60cc2-8591-45b0-a6de-f224844f3eb8","executionInfo":{"status":"ok","timestamp":1691600527016,"user_tz":-60,"elapsed":14273,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2752825df9b4efdab000a81c8062aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fefb96a1f10414c9e2181301bf9b638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70bc9d76cff24572a1d52c059aaf74c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a5b015646f487589858604f73e43c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854efc088dab405c8102eb27879572a6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def tokenize_data(data, tokenizer):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for text in data['TranslatedText']:\n","    encoded_text = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens = True,\n","        max_length=256,\n","        pad_to_max_length=True,\n","        return_attention_mask=True,\n","        return_tensors = 'pt'\n","    )\n","    input_ids.append(encoded_text['input_ids'])\n","    attention_masks.append(encoded_text['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim = 0)\n","  labels = torch.tensor(data['Label'].map({'Supporter':0,'Against':1,'Manipulator':2,'Neutral': 3}).tolist())\n","\n","  return input_ids, attention_masks, labels"],"metadata":{"id":"U-uugpEQo0LU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_ids, train_attention_masks, train_labels = tokenize_data(train_df, tokenizer)\n","val_input_ids, val_attention_masks, val_labels = tokenize_data(val_df, tokenizer)\n","test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df, tokenizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4IytGcepE2H","outputId":"19945bba-3b08-4bac-9eb1-ff8251df3f1d","executionInfo":{"status":"ok","timestamp":1691600539599,"user_tz":-60,"elapsed":654,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","test_loader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"_vlgpd1ipXYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model2.to(device)\n","optimizer = AdamW(model2.parameters(), lr=2e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDgMP6xhphF8","outputId":"96b0b03a-65ce-443a-9ede-019ed3391342","executionInfo":{"status":"ok","timestamp":1691600552977,"user_tz":-60,"elapsed":5421,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["num_epochs = 6\n","for epoch in range(num_epochs):\n","    model2.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, label = batch\n","        input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n","\n","        outputs = model2(input_ids, attention_mask = attention_mask, labels = label)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    #Validation\n","    model2.eval()\n","    with torch.no_grad():\n","        total_val_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","        predictions = []\n","        ground_truth = []\n","        for batch in val_loader:\n","            input_ids, attention_mask, label = batch\n","            input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n","            outputs = model2(input_ids, attention_mask = attention_mask, labels = label)\n","            loss = outputs.loss\n","            total_val_loss += loss.item()\n","\n","            logits = outputs.logits\n","            predicted_labels = torch.argmax(logits, dim=1)\n","\n","            predictions.extend(predicted_labels.cpu().numpy())\n","            ground_truth.extend(label.cpu().numpy())\n","\n","            correct_predictions += (predicted_labels == label).sum().item()\n","            total_predictions += label.size(0)\n","\n","        test_accuracy = correct_predictions/ total_predictions\n","        average_val_loss = total_val_loss/len(val_loader)\n","        print(f\"Validation Accuracy: {test_accuracy:.4f}\")\n","        print(f\"Average Validation Loss: {average_val_loss:.4f}\")\n","\n","         #Calculate the accuracy and F1 score for each label\n","        target_names = ['Supporter','Against','Manipulator','Neutral']\n","        report = classification_report(ground_truth, predictions, target_names=target_names,output_dict=True)\n","\n","        print(\"Classification Report:\")\n","        print(pd.DataFrame(report).transpose())"],"metadata":{"id":"6rZuzSIoqDYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691601256481,"user_tz":-60,"elapsed":615068,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"ff4c2319-f652-4507-94ad-cc02d42c8867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.6331\n","Average Validation Loss: 1.2096\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.633136  1.000000  0.775362  107.000000\n","Against        0.000000  0.000000  0.000000   15.000000\n","Manipulator    0.000000  0.000000  0.000000   30.000000\n","Neutral        0.000000  0.000000  0.000000   17.000000\n","accuracy       0.633136  0.633136  0.633136    0.633136\n","macro avg      0.158284  0.250000  0.193841  169.000000\n","weighted avg   0.400861  0.633136  0.490910  169.000000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.7041\n","Average Validation Loss: 0.9362\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.762295  0.869159  0.812227  107.000000\n","Against        0.000000  0.000000  0.000000   15.000000\n","Manipulator    0.553191  0.866667  0.675325   30.000000\n","Neutral        0.000000  0.000000  0.000000   17.000000\n","accuracy       0.704142  0.704142  0.704142    0.704142\n","macro avg      0.328872  0.433956  0.371888  169.000000\n","weighted avg   0.580836  0.704142  0.634130  169.000000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.7870\n","Average Validation Loss: 0.8162\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.786260  0.962617  0.865546  107.000000\n","Against        1.000000  0.200000  0.333333   15.000000\n","Manipulator    0.781250  0.833333  0.806452   30.000000\n","Neutral        0.666667  0.117647  0.200000   17.000000\n","accuracy       0.786982  0.786982  0.786982    0.786982\n","macro avg      0.808544  0.528399  0.551333  169.000000\n","weighted avg   0.792311  0.786982  0.740870  169.000000\n","Validation Accuracy: 0.7219\n","Average Validation Loss: 0.7848\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.811321  0.803738  0.807512  107.000000\n","Against        0.833333  0.333333  0.476190   15.000000\n","Manipulator    0.600000  0.900000  0.720000   30.000000\n","Neutral        0.333333  0.235294  0.275862   17.000000\n","accuracy       0.721893  0.721893  0.721893    0.721893\n","macro avg      0.644497  0.568091  0.569891  169.000000\n","weighted avg   0.727680  0.721893  0.709090  169.000000\n","Validation Accuracy: 0.7456\n","Average Validation Loss: 0.8709\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.786325  0.859813  0.821429  107.000000\n","Against        0.750000  0.400000  0.521739   15.000000\n","Manipulator    0.766667  0.766667  0.766667   30.000000\n","Neutral        0.357143  0.294118  0.322581   17.000000\n","accuracy       0.745562  0.745562  0.745562    0.745562\n","macro avg      0.665034  0.580149  0.608104  169.000000\n","weighted avg   0.736439  0.745562  0.734928  169.000000\n","Validation Accuracy: 0.8107\n","Average Validation Loss: 0.9947\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.808000  0.943925  0.870690  107.000000\n","Against        1.000000  0.400000  0.571429   15.000000\n","Manipulator    0.827586  0.800000  0.813559   30.000000\n","Neutral        0.666667  0.352941  0.461538   17.000000\n","accuracy       0.810651  0.810651  0.810651    0.810651\n","macro avg      0.825563  0.624217  0.679304  169.000000\n","weighted avg   0.814301  0.810651  0.792829  169.000000\n"]}]},{"cell_type":"code","source":["model2.eval()\n","with torch.no_grad():\n","  total_test_loss = 0.0\n","  correct_predictions = 0\n","  total_predictions = 0\n","  predictions = []\n","  ground_truth = []\n","  for batch in test_loader:\n","    input_ids, attention_mask, label = batch\n","    input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n","    outputs = model2(input_ids, attention_mask=attention_mask, labels = label)\n","    loss = outputs.loss\n","    total_test_loss += loss.item()\n","\n","    logits = outputs.logits\n","    predicted_labels = torch.argmax(logits, dim=1)\n","\n","    predictions.extend(predicted_labels.cpu().numpy())\n","    ground_truth.extend(label.cpu().numpy())\n","\n","    correct_predictions += (predicted_labels == label).sum().item()\n","    total_predictions += label.size(0)\n","\n","test_accuracy = correct_predictions/ total_predictions\n","average_test_loss = total_test_loss/ len(test_loader)\n","\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Average Test Loss: {average_test_loss:.4f}\")\n","\n"," #Calculate the accuracy and F1 score for each label\n","target_names = ['Supporter','Against','Manipulator','Neutral']\n","report = classification_report(ground_truth, predictions, target_names=target_names,output_dict=True)\n","\n","print(\"Classification Report:\")\n","print(pd.DataFrame(report).transpose())\n"],"metadata":{"id":"pQ2ewa78t52P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691601563064,"user_tz":-60,"elapsed":10388,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"23870916-f623-49d0-91e1-b5bcfbd596e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7075\n","Average Test Loss: 1.0113\n","Classification Report:\n","              precision    recall  f1-score     support\n","Supporter      0.727273  0.888889  0.800000  135.000000\n","Against        0.666667  0.250000  0.363636   16.000000\n","Manipulator    0.666667  0.588235  0.625000   34.000000\n","Neutral        0.545455  0.222222  0.315789   27.000000\n","accuracy       0.707547  0.707547  0.707547    0.707547\n","macro avg      0.651515  0.487337  0.526106  212.000000\n","weighted avg   0.689823  0.707547  0.677333  212.000000\n"]}]},{"cell_type":"code","source":["completetweets = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/completetweets.csv')"],"metadata":{"id":"Q3Z_wNce-Iu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n"],"metadata":{"id":"x79JYvUvCqh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dir(train_df))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3cHIUeLD6jA","executionInfo":{"status":"ok","timestamp":1691513588858,"user_tz":-60,"elapsed":471,"user":{"displayName":"mathew abraham","userId":"12771230762283735045"}},"outputId":"1fccb2a2-870b-46e8-8b83-de21d325f42d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Datetime', 'Label', 'LanguageUsed', 'T', 'Text', 'TranslatedText', 'TweetID', 'Username', '_AXIS_LEN', '_AXIS_ORDERS', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__dataframe__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_by_level', '_agg_examples_doc', '_agg_summary_and_see_also_doc', '_align_frame', '_align_series', '_append', '_arith_method', '_as_manager', '_attrs', '_box_col_values', '_can_fast_transpose', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_combine_frame', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_sliced', '_convert', '_count_level', '_data', '_dir_additions', '_dir_deletions', '_dispatch_frame_op', '_drop_axis', '_drop_labels_or_levels', '_ensure_valid_index', '_find_valid_index', '_flags', '_from_arrays', '_get_agg_axis', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cleaned_column_resolvers', '_get_column_array', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_getitem_bool_array', '_getitem_multilevel', '_gotitem', '_hidden_attrs', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_info_repr', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_copy', '_is_homogeneous_type', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_iset_item_mgr', '_iset_not_inplace', '_item_cache', '_iter_column_arrays', '_ixs', '_join_compat', '_logical_func', '_logical_method', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_mgr', '_min_count_stat_function', '_needs_reindex_multi', '_protect_consolidate', '_reduce', '_reduce_axis1', '_reindex_axes', '_reindex_columns', '_reindex_index', '_reindex_multi', '_reindex_with_indexers', '_rename', '_replace_columnwise', '_repr_data_resource_', '_repr_fits_horizontal_', '_repr_fits_vertical_', '_repr_html_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_sanitize_column', '_series', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_item_frame_value', '_set_item_mgr', '_set_value', '_setitem_array', '_setitem_frame', '_setitem_slice', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take', '_take_with_is_copy', '_to_dict_of_blocks', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at', 'at_time', 'attrs', 'axes', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'columns', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'hashtags', 'head', 'hist', 'iat', 'id', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert', 'interpolate', 'isetitem', 'isin', 'isna', 'isnull', 'items', 'iteritems', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'likeCount', 'loc', 'lookup', 'lt', 'mad', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'mentionedUsers', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'quoteCount', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'retweetCount', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'squeeze', 'stack', 'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_orc', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'values', 'var', 'viewCount', 'where', 'xs']\n"]}]},{"cell_type":"code","source":["torch.save(model2.state_dict(),'math_BERTweets.pth')\n","\n","# Save other relevant information\n","#additional_info = {\n"," #   'hyperparameters': {\n","  #      'learning_rate': 0.001,\n","   #     'batch_size': 32,\n","    #    'num_epochs': 10\n","    #},\n","    #'tokenizer_config': tokenizer.save_pretrained(\"tokenizer_directory\"),\n","    #'training_data_info': {\n","     #   'dataset_size': len(train_df),\n","    #},\n","    #'optimizer_state': optimizer.state_dict()\n","    #}\n","\n","#with open('additional_info.json', 'w') as info_file:\n"," #   json.dump(additional_info, info_file)"],"metadata":{"id":"NkhH7jShA4KS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_path = '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/'\n","\n","model2.save_pretrained(save_path)\n","tokenizer.save_pretrained(save_path)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9gn8ThRHC_a","executionInfo":{"status":"ok","timestamp":1691601736604,"user_tz":-60,"elapsed":9016,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"6e867f7d-5c7f-4d1d-8a96-4a81f59c4dbb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/FineTunedModels/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/vocab.json',\n"," '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/merges.txt',\n"," '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/added_tokens.json',\n"," '/content/drive/MyDrive/Colab Notebooks/FineTunedModels/tokenizer.json')"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/Colab Notebooks/FineTunedModels/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mPUeeIJUrPA","executionInfo":{"status":"ok","timestamp":1691601823650,"user_tz":-60,"elapsed":14968,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"93487a66-d4bb-425f-f5b3-8177f72fbfe3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/FineTunedModels/ and are newly initialized: ['encoder.layer.7.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.14.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'classifier.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/Colab Notebooks/FineTunedModels/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"iU8FjNL3YQfR","executionInfo":{"status":"error","timestamp":1691603131764,"user_tz":-60,"elapsed":253,"user":{"displayName":"Mathew Plavelil Abraham","userId":"08011009432654902676"}},"outputId":"74272d62-6249-414e-978c-5d655c7a765d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-21c930ef856d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/FineTunedModels/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1839\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1842\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m             raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             raise ValueError(\n\u001b[1;32m    215\u001b[0m                 \u001b[0;34mf\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"]}]}]}